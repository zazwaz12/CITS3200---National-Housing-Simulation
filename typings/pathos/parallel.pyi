"""
This type stub file was generated by pyright.
"""

from pathos.abstract_launcher import AbstractWorkerPool

"""
This module contains map and pipe interfaces to the parallelpython (pp) module.

Pipe methods provided:
    pipe        - blocking communication pipe             [returns: value]
    apipe       - asynchronous communication pipe         [returns: object]

Map methods provided:
    map         - blocking and ordered worker pool        [returns: list]
    imap        - non-blocking and ordered worker pool    [returns: iterator]
    uimap       - non-blocking and unordered worker pool  [returns: iterator]
    amap        - asynchronous worker pool                [returns: object]


Usage
=====

A typical call to a pathos pp map will roughly follow this example:

    >>> # instantiate and configure the worker pool
    >>> from pathos.pp import ParallelPool
    >>> pool = ParallelPool(nodes=4)
    >>>
    >>> # do a blocking map on the chosen function
    >>> print(pool.map(pow, [1,2,3,4], [5,6,7,8]))
    >>>
    >>> # do a non-blocking map, then extract the results from the iterator
    >>> results = pool.imap(pow, [1,2,3,4], [5,6,7,8])
    >>> print("...")
    >>> print(list(results))
    >>>
    >>> # do an asynchronous map, then get the results
    >>> results = pool.amap(pow, [1,2,3,4], [5,6,7,8])
    >>> while not results.ready():
    ...     time.sleep(5); print(".", end=' ')
    ...
    >>> print(results.get())
    >>>
    >>> # do one item at a time, using a pipe
    >>> print(pool.pipe(pow, 1, 5))
    >>> print(pool.pipe(pow, 2, 6))
    >>>
    >>> # do one item at a time, using an asynchronous pipe
    >>> result1 = pool.apipe(pow, 1, 5)
    >>> result2 = pool.apipe(pow, 2, 6)
    >>> print(result1.get())
    >>> print(result2.get())


Notes
=====

This worker pool leverages the parallelpython (pp) module, and thus
has many of the limitations associated with that module. The function f and
the sequences in args must be serializable. The maps in this worker pool
have full functionality when run from a script, but may be somewhat limited
when used in the python interpreter. Both imported and interactively-defined
functions in the interpreter session may fail due to the pool failing to
find the source code for the target function. For a work-around, try:

    >>> # instantiate and configure the worker pool
    >>> from pathos.pp import ParallelPool
    >>> pool = ParallelPool(nodes=4)
    >>>
    >>> # wrap the function, so it can be used interactively by the pool
    >>> def wrapsin(*args, **kwds):
    >>>      from math import sin
    >>>      return sin(*args, **kwds)
    >>>
    >>> # do a blocking map using the wrapped function
    >>> results = pool.map(wrapsin, [1,2,3,4,5])

"""
__all__ = ["ParallelPool", "stats"]
_ParallelPool__STATE = ...

def stats(pool=...):  # -> str:
    "return a string containing stats response from the pp.Server"
    ...

class ParallelPool(AbstractWorkerPool):
    """
    Mapper that leverages parallelpython (i.e. pp) maps.
    """

    def __init__(self, *args, **kwds) -> None:
        """\nNOTE: if number of nodes is not given, will autodetect processors.
        \nNOTE: if a tuple of servers is not provided, defaults to localhost only.
        \nNOTE: additional keyword input is optional, with:
            id          - identifier for the pool
            servers     - tuple of pp.Servers
        """
        ...
    if AbstractWorkerPool.__init__.__doc__:
        ...
    clear = ...
    def map(self, f, *args, **kwds):  # -> list[Any]:
        ...

    def imap(self, f, *args, **kwds):  # -> Generator[Any, None, None]:
        ...

    def uimap(self, f, *args, **kwds):  # -> Generator[Any, Any, None]:
        ...

    def amap(self, f, *args, **kwds):  # -> MapResult:
        ...

    def pipe(self, f, *args, **kwds):  # -> Any:
        ...

    def apipe(self, f, *args, **kwds):  # -> ApplyResult:
        ...

    def __repr__(self):  # -> str:
        ...

    def restart(self, force=...):  # -> None:
        "restart a closed pool"
        ...

    def close(self):  # -> None:
        "close the pool to any new jobs"
        ...

    def terminate(self):  # -> None:
        "a more abrupt close"
        ...

    def join(self):  # -> None:
        "cleanup the closed worker processes"
        ...
    ncpus = ...
    nodes = ...
    servers = ...
    __state__ = ...

ParallelPythonPool = ParallelPool
